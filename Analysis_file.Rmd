---
title: "Analysis_file"
author: "Chenxi Li"
output: pdf_document
date: "2025-05-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Prepare necessary packages

In this section, we are going to load necessary packages for our further research.
```{R}
# remove objects
rm(list = ls())
# detach all libraries
detachAllPackages <- function() {
  basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
  package.list <- search()[ifelse(unlist(gregexpr("package:", search())) == 1, TRUE, FALSE)]
  package.list <- setdiff(package.list, basic.packages)
  if (length(package.list) > 0) for (package in package.list) detach(package, character.only = TRUE)
}
detachAllPackages()

# load libraries
pkgTest <- function(pkg) {
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"), pkgTest)

lapply(c("tidyverse",
         "cld3", # for language detect
         "ggplot2",
         "MetBrewer", # for visualisation color
         "stopwords", # for data tidy, to filter stopword
         "ggwordcloud", # for wordcloud
         "RColorBrewer",
         "tidytext"
         ), pkgTest)

options(scipen = 200)

set.seed(42)
```


## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{R}
# Create a temporary path for data downloading
temp <- tempdir()
data <- file.path(temp, "Cyberpunk-2077-steam-reviews-as-of-aug-8-2024.zip")
unzip <- file.path(temp, "unzip")
dir.create(data, showWarnings = FALSE)

# Judge if there is already exist path. If yes, delete the old one.
if (dir.exists(data)) {
  unlink(data, recursive = TRUE)
}

# Data downloading
kaggle <- paste0(
  "kaggle datasets download -d filas1212/cyberpunk-2077-steam-reviews-as-of-aug-8-2024 ",
  "--path \"", temp, "\""
)
system(kaggle)

# Data unzipping and checking the file name
unzip(zipfile = data, exdir = unzip)
list.files(unzip)
```

```{R}
raw <- read_csv(file.path(unzip, "Cyberpunk_2077_Steam_Reviews.csv"))
head(df)
```

## Data Tidy

In this section, we are going to do the data tidy.

The first step is to clean several redunctant variables like ReviewID and SteamID due to privacy policy.

And also we notice in the previous section that there are several kinds of language in the data frame. So we are going to see the distribution of different language and then I only extract English comments as the main data source of this analysis for convinience.

```{R}
# Remove row ReviewID and SteamID, this is private information
df <- raw %>%
  select(-ReviewID, -SteamID)

# Detect language in Review
df <- df %>%
  mutate(Language = cld3::detect_language(Review))

# Count the frequency of language
lang_counts <- df %>%
  count(Language, sort = TRUE) %>%
  slice_max(n, n = 10)
```
```{r pressure, echo=FALSE}
# Visulize the distribution of language
ggplot(lang_counts, aes(x = reorder(Language, n), y = n)) +
  geom_col(fill = met.brewer("Cassatt2")[7],
           color = "black") +
  labs(
    title = "Language Distribution",
    x = "Language",
    y = "Frequency",
  ) + 
  theme_bw()
```

Only analysis for English comments:

```{R}
df_en <- df %>%
  filter(Language == "en")
dim(df_en)
```

There are still 257662 observations in English comments, which will lead a very slow reaction during analysis, at least on my computer :(, so I decided to sample the data set to make it accessibility.

The strategy of sampling in this case will be stratified sampling by month, the reasons are:

  (1) 

Create a function for data tidy

```{R}
# Prepare the stopwords library
stopwords <- c(stopwords("en"), "game", "games", "can")

# Function to text data tidy 
review_tidy <- function(text) {
    text <- tolower(text)                 # Change to lower writing style
    text <- gsub("[[:punct:]]", "", text) # Delete character like !@
    text <- gsub("\\s+", " ", text)       # Delete extra space
    text <- trimws(text)                  # Delete extra lines
    text <- gsub("[0-9]+", "", text)      # Drop numbers
    words <- unlist(strsplit(text, "\\s+"))
    cleaned <- words[!tolower(words) %in% stopwords]
    return(paste(cleaned, collapse = " "))
}

df_en <- df_en %>%
  mutate(Review = sapply(Review, review_tidy))
```


```{R}
words <- df_en %>%
  select(Review) %>%
  unnest_tokens(word, Review)

word_freq <- words %>% 
  count(word, sort = TRUE)
print(word_freq)
```

```{r wordcloud, fig.width = 7, fig.height = 7}

```



Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
