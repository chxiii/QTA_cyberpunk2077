---
title: "Analysis_file"
author: "Chenxi Li"
output: pdf_document
date: "2025-05-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Prepare necessary packages

In this section, we are going to load necessary packages for our further research.
```{R}
# remove objects
rm(list = ls())
# detach all libraries
detachAllPackages <- function() {
  basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
  package.list <- search()[ifelse(unlist(gregexpr("package:", search())) == 1, TRUE, FALSE)]
  package.list <- setdiff(package.list, basic.packages)
  if (length(package.list) > 0) for (package in package.list) detach(package, character.only = TRUE)
}
detachAllPackages()

# load libraries
pkgTest <- function(pkg) {
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"), pkgTest)

lapply(c("tidyverse",
         "cld3", # for language detect
         "ggplot2",
         "MetBrewer", # for visualisation color
         "stopwords", # for data tidy, to filter stopword
         "ggwordcloud", # for wordcloud
         "RColorBrewer",
         "tidytext",
         "dplyr", # for sampling
         "lubridate", # for date converse
         "scales"
         ), pkgTest)

options(scipen = 200)

set.seed(42)
```


## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{R}
# Create a temporary path for data downloading
temp <- tempdir()
data <- file.path(temp, "Cyberpunk-2077-steam-reviews-as-of-aug-8-2024.zip")
unzip <- file.path(temp, "unzip")
dir.create(data, showWarnings = FALSE)

# Judge if there is already exist path. If yes, delete the old one.
if (dir.exists(data)) {
  unlink(data, recursive = TRUE)
}

# Data downloading
kaggle <- paste0(
  "kaggle datasets download -d filas1212/cyberpunk-2077-steam-reviews-as-of-aug-8-2024 ",
  "--path \"", temp, "\""
)
system(kaggle)

# Data unzipping and checking the file name
unzip(zipfile = data, exdir = unzip)
list.files(unzip)
```

```{R}
raw <- read_csv(file.path(unzip, "Cyberpunk_2077_Steam_Reviews.csv"))
head(df)
```

## Data Tidy

In this section, we are going to do the data tidy.

The first step is to clean several redunctant variables like ReviewID and SteamID due to privacy policy.

And also we notice in the previous section that there are several kinds of language in the data frame. So we are going to see the distribution of different language and then I only extract English comments as the main data source of this analysis for convenience.

```{R}
# Remove row ReviewID and SteamID, this is private information
df <- raw %>%
  select(-ReviewID, -SteamID)

# Detect language in Review
df <- df %>%
  mutate(Language = cld3::detect_language(Review))

# Count the frequency of language
lang_counts <- df %>%
  count(Language, sort = TRUE) %>%
  slice_max(n, n = 10)
```

English and Chinese are two most popular languages used in all comment players. Going further, we can speculate that the largest communities for Cyberpunk 2077 are English and Chinese communities.

```{R}
# Visulize the distribution of language
ggplot(lang_counts, aes(x = reorder(Language, n), y = n)) +
  geom_col(fill = met.brewer("Cassatt2")[7],
           color = "black") +
  labs(
    title = "Language Distribution",
    x = "Language",
    y = "Frequency",
  ) + 
  theme_bw() + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
```

In this case, we only analysis for English comments, but probably one day I might do the Chinese part as well :)

```{R}
df_en <- df %>%
  filter(Language == "en")
dim(df_en)
```

There are still 257662 observations in English comments, which will lead a very slow reaction during analysis, at least on my computer :(, so I decided to sample the data set to make it accessibility.

The strategy of sampling in this case will be stratified sampling by month, the reasons are:

(1) Based on empirical observations and the theories of game critics, there is a trend of reviews of the game Cyberpunk 2077 differentiating over time, from mixed reviews at the beginning of the game to positive reviews today, so time is an important dimension for understanding the shift in reviews.

(2) Compared with other sampling methods, such as simple random sampling, or non-random cluster sampling, encounter sampling, etc., stratified sampling is more conducive to controlling the number of reviews at each time point and conducting subsequent statistical analysis.

```{R}
# Prepared data for proportional sampling. Extract Date Posted into the format as Year-Month for sampling.
df_en <- df_en %>%
  mutate(
    date_posted = parse_date_time(`Date Posted`, 
                                  # "mdy" -> for 08-24-24
                                  # "d-b-y" -> for 18-Aug-24
                                  # "d-B-y" -> for 18-August-24
                                  orders = c("mdy", "d-b-y", "d-B-y")),
    year_month = format(date_posted, "%Y-%m")
  )
```

Before we sampling, it is necessary to see the tendency of comments amount with time. So here is a visualization of time and comments ammount:

We can find except 12/2020, which is the publication date of the cyberpunk 2077, there are also several peak review moment including 11/2021, 09/2022, 09/2023, which correspond to major update patch 1.3, 1.6, and 2.0 respectively. The detailed patch timeline can be found in this website: <https://store.steampowered.com/news/app/1091500>

From the plot below we can analysis that every patch brought an increase in discussion, even before the patch relase, there were already discussion increase happened in community.

```{r pressure, echo=FALSE}
# Prepare data set for the plot, due to it is group by month
df_monthly <- df_en %>%
  filter(!is.na(year_month)) %>%
  group_by(year_month) %>%
  summarise(count = n()) %>%
  arrange(year_month) %>%
  mutate(date = as.Date(paste0(year_month, "-01")))

# Prepare data set for key patch
key_time <- data.frame(
  x = as.Date(c("2020-12-01", "2021-11-01", "2022-09-01", "2023-09-01")),
  y = rep(max(df_monthly$count) * 0.95, 4), 
  label = c("Patch 1.0", "Patch 1.3", "Patch 1.6", "Patch 2.0")
)

# Visualize the distribution of comments and mark the key patch
ggplot(df_monthly, aes(x = date, y = count)) +
  geom_line(color = met.brewer("Monet")[2], size = 1) +
  geom_point(color = met.brewer("Monet")[1], size = 1.5, shape = 0) +
  geom_vline(data = key_time, aes(xintercept = x), 
             linetype = "dashed", color = met.brewer("Monet")[1], alpha = 0.6) +
  geom_text(data = key_time,
            aes(x = x, y = y, label = label),
            vjust = -0.5, hjust = -0.1,
            color = met.brewer("Monet")[1], size = 3.5) +
  labs(title = "Review amount and key patch",
       x = "",
       y = "") +
  scale_x_date(date_labels = "%Y-%m", date_breaks = "3 months") +
  theme_bw(base_size = 13) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
```

Next, we are going to sample the population to make the data set more clear.

```{R}
# Proportional sampling
df_sam <- df_en %>%
  group_by(year_month) %>%
  mutate(group_n = n()) %>%
  sample_frac(0.05) %>%
  ungroup()

dim(df_sam)
```



Create a function for data tidy

```{R}
# Prepare the stopwords library
stopwords <- c(stopwords("en"), "game", "games", "can", "yes")

# Function to text data tidy 
review_tidy <- function(text) {
    text <- tolower(text)                 # Change to lower writing style
    text <- gsub("[[:punct:]]", "", text) # Delete character like !@
    text <- gsub("\\s+", " ", text)       # Delete extra space
    text <- trimws(text)                  # Delete extra lines
    text <- gsub("[0-9]+", "", text)      # Drop numbers
    words <- unlist(strsplit(text, "\\s+"))
    cleaned <- words[!tolower(words) %in% stopwords]
    return(paste(cleaned, collapse = " "))
}

df_en <- df_en %>%
  mutate(Review = sapply(Review, review_tidy))
```


```{R}
words <- df_sam %>%
  select(Review) %>%
  unnest_tokens(word, Review)

word_freq <- words %>% 
  count(word, sort = TRUE)
print(word_freq)
```

```{r wordcloud, fig.width = 7, fig.height = 7}
ggplot(word_freq, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 20 Most Frequent Words in Reviews",
       x = "Word",
       y = "Frequency") +
  theme_minimal(base_size = 13)
```



Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
